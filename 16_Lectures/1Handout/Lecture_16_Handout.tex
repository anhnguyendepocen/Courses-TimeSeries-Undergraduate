\documentclass[12] {article}
\usepackage{setspace}
\usepackage{amssymb,amsmath}


\begin{document}
\onehalfspace

\title{Lecture 16}
\date{}
\maketitle


\section{Parametric Bootstrap}

Suppose that we have a statistical model $f(x; \theta)$ for data $X$. Suppose that we have an estimator $\widehat{\theta}(x)$ for the parameter $\theta$ (for example, by doing OLS or ML). \\

The estimator $\widehat{\theta}(x)$ is a random variable, as it depends on the data. If $\theta$ were known, we could approximate the distribution of $\widehat{\theta}(x)$ by Monte-Carlo methods: generate $I$ independent draws from $x \sim f(x;\theta)$, and evaluate $\widehat{\theta}(x)$ over the $I$ new data sets that we have generated.  \\


The true parameter that generated the data is unknown; hence it is not possible to generate draws from the model $f(x;\theta)$. However, since there is an estimator for $\theta$, one could perform the Monte-Carlo approximation exercise described above by using draws from the model:

\[ f(x; \widehat{\theta}(x)).\]

If we do this, we will effectively end with $I$ estimators (one for each new data set). If the sample size is large and the parametric model $f(x;\theta)$ varies smoothly with $\theta$, the c.d.f. based on the I estimators can be shown to be a reasonable approximate for the distribution of $\widehat{\theta}$. \\

The Monte-Carlo type exercise describe above (which you will implement in the homework) is known as the \underline{parametric bootstrap} and it is a general statistical technique used to estimate the distribution of $\widehat{\theta}(x)$. 




 
 
 




 

\end{document}